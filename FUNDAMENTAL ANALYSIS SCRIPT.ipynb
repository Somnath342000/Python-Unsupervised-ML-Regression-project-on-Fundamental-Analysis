{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYTHON FUNDAMENTAL STOCK ANALYSIS USING UNSUPERVISED MACHINE LEARNING REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import mean\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sqlalchemy\n",
    "sqlalchemy.__version__\n",
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "SAMPLEDATA=pd.read_csv(filepath_or_buffer=\"G:/IVY/data/MACHINE_LEARNING/stock.csv\", \n",
    "                              sep=',', encoding='latin-1')\n",
    "print(type(SAMPLEDATA))\n",
    "pd.set_option\n",
    "\n",
    "pd.options.display.max_columns=50\n",
    "pd.options.display.max_rows=100\n",
    "print('Shape before deleting duplicate values:', SAMPLEDATA.shape)\n",
    "\n",
    "SAMPLEDATA=SAMPLEDATA.drop_duplicates()\n",
    "print('Shape After deleting duplicate values:', SAMPLEDATA.shape)\n",
    "\n",
    "SAMPLEDATA.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the problem statement: \n",
    "* Target Variable: Graham Price\n",
    "* Predictors: Intrinsic Value, Book Value, EPS, CMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the distribution of Target variable\n",
    "* If target variable's distribution is too skewed then the predictive modeling will not be possible.\n",
    "* Bell curve is desirable but slightly positive skew or negative skew is also fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Creating Bar chart as the Target variable is Continuous\n",
    "SAMPLEDATA['Graham Price'].hist(figsize=(5,4),color='Orange',bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLEDATA.describe(include='all')\n",
    "print(\"No of null Values per Column :- \")\n",
    "print(SAMPLEDATA.isnull().sum())\n",
    "print(\"***************************************************************\")\n",
    "print(\"No of Unique Values per Column :- \")\n",
    "SAMPLEDATA.nunique()\n",
    "SAMPLEinfo=SAMPLEDATA.info()\n",
    "SAMPLEDATA.columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## missing value treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLEDATA['Intrinsic Value'].fillna(value=SAMPLEDATA['Intrinsic Value'].median(), inplace=True) # Continious variable\n",
    "SAMPLEDATA['Graham Price'].fillna(value=SAMPLEDATA['Graham Price'].median(), inplace=True) # Continious variable\n",
    "print(SAMPLEDATA.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE ENGINEARING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLEDATA['P/B Ratio']=SAMPLEDATA['CMP']/SAMPLEDATA['Book Value']\n",
    "SAMPLEDATA['P/E Ratio']=SAMPLEDATA['CMP']/SAMPLEDATA['EPS ']\n",
    "SAMPLEDATA['Debt/Asset Ratio']=SAMPLEDATA['Debt']/SAMPLEDATA['Assets']\n",
    "SAMPLEDATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA INTERPRETATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR CATEGORICAL VARIABLES\n",
    "def PlotBarCharts(inpData, colsToPlot):\n",
    "    %matplotlib inline\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Generating multiple subplots\n",
    "    fig, subPlot=plt.subplots(nrows=1, ncols=len(colsToPlot), figsize=(20,5))\n",
    "    fig.suptitle('Bar charts of: '+ str(colsToPlot))\n",
    "\n",
    "    for colName, plotNumber in zip(colsToPlot, range(len(colsToPlot))):\n",
    "        inpData.groupby(colName).size().plot(kind='bar',color='green',ax=subPlot[plotNumber])\n",
    "\n",
    "# Calling the Function\n",
    "PlotBarCharts(inpData=SAMPLEDATA, colsToPlot=['G Factor', 'Piotski Scrore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping the SibSP values 3,4,5,8 in one single bucket as 3\n",
    "SAMPLEDATA['G Factor'][SAMPLEDATA['G Factor']>=8]=8\n",
    "PlotBarCharts(inpData=SAMPLEDATA, colsToPlot=['G Factor', 'Piotski Scrore'])\n",
    "\n",
    "# exporting the image to the local disk\n",
    "plt.show()\n",
    "plt.savefig('Column Charts.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR CONTINIOUS VARIABLES\n",
    "SAMPLEDATA.hist(['Altman Z Scr', 'Enterprise Value',\n",
    "       'Intrinsic Value', 'Sales', 'Debt', 'Assets', 'Working Capital',\n",
    "       'Book Value', 'Capital Employed', 'Piotski Scrore', 'Leverage',\n",
    "       'Reserves', 'EPS ','CMP','P/B Ratio', 'P/E Ratio', 'Debt/Asset Ratio'], figsize=(18,15),color='red')\n",
    "\n",
    "# exporting the image to the local disk\n",
    "plt.show()\n",
    "plt.savefig('Histogram Charts.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual exploration of relationship between variables\n",
    "* Continuous Vs Continuous ---- Scatter Plot\n",
    "* Categorical Vs Continuous---- Box Plot\n",
    "* Categorical Vs Categorical---- Grouped Bar Plots\n",
    "\n",
    "## Statistical measurement of relationship strength between variables\n",
    "* Continuous Vs Continuous ---- Correlation matrix\n",
    "* Categorical Vs Continuous---- ANOVA test\n",
    "* Categorical Vs Categorical--- Chi-Square test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ContinuousCols=['Altman Z Scr', 'Enterprise Value',\n",
    "       'Intrinsic Value', 'Sales', 'Debt', 'Assets', 'Working Capital',\n",
    "       'Book Value', 'Capital Employed', 'Piotski Scrore', 'Leverage',\n",
    "       'Reserves', 'EPS ','CMP','P/B Ratio', 'P/E Ratio', 'Debt/Asset Ratio']\n",
    "\n",
    "# Plotting scatter chart for each predictor vs the target variable\n",
    "for predictor in ContinuousCols:\n",
    "    SAMPLEDATA.plot.scatter(x=predictor, y='Graham Price', figsize=(5,2), title=predictor+\" VS \"+ 'Graham Price')\n",
    "\n",
    "# exporting the image to the local disk\n",
    "plt.show()\n",
    "plt.savefig('Scatter Charts.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating correlation matrix\n",
    "ContinuousCols1=['Altman Z Scr', 'Enterprise Value', 'Graham Price',\n",
    "       'Intrinsic Value', 'Sales', 'Debt', 'Assets', 'Working Capital',\n",
    "       'Book Value', 'Capital Employed', 'Piotski Scrore', 'Leverage',\n",
    "       'Reserves', 'EPS ','CMP','P/B Ratio', 'P/E Ratio', 'Debt/Asset Ratio']\n",
    "\n",
    "# Creating the correlation matrix\n",
    "CorrelationData=SAMPLEDATA[ContinuousCols1].corr()\n",
    "CorrelationData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering only those columns where absolute correlation > 0.5 with Target Variable\n",
    "# reduce the 0.5 threshold if no variable is selected\n",
    "CorrelationData['Graham Price'][abs(CorrelationData['Graham Price']) > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(CorrelationData['Graham Price'])>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for Categorical Target Variable  and continuous predictors\n",
    "CategoricalColsList=['G Factor', 'Piotski Scrore']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, PlotCanvas=plt.subplots(nrows=1, ncols=len(CategoricalColsList), figsize=(20,5))\n",
    "\n",
    "# Creating box plots for each continuous predictor against the Target Variable \n",
    "for PredictorCol , i in zip(CategoricalColsList, range(len(CategoricalColsList))):\n",
    "    SAMPLEDATA.boxplot(column='Enterprise Value', by=PredictorCol, figsize=(10,10), vert=True, ax=PlotCanvas[i])\n",
    "\n",
    "# exporting the image to the local disk\n",
    "plt.show()\n",
    "plt.savefig('Box Charts.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Feature Selection (Categorical Vs Continuous) using ANOVA test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to find the statistical relationship with all the categorical variables\n",
    "def FunctionAnova(inpData, TargetVariable, CategoricalPredictorList):\n",
    "    from scipy.stats import f_oneway\n",
    "\n",
    "    # Creating an empty list of final selected predictors\n",
    "    SelectedPredictors=[]\n",
    "    \n",
    "    print('##### ANOVA Results ##### \\n')\n",
    "    for predictor in CategoricalPredictorList:\n",
    "        CategoryGroupLists=inpData.groupby(predictor)[TargetVariable].apply(list)\n",
    "        AnovaResults = f_oneway(*CategoryGroupLists)\n",
    "        \n",
    "        # If the ANOVA P-Value is <0.05, that means we reject H0\n",
    "        if (AnovaResults[1] < 0.05):\n",
    "            print(predictor, 'is correlated with', TargetVariable, '| P-Value:', AnovaResults[1])\n",
    "            SelectedPredictors.append(predictor)\n",
    "        else:\n",
    "            # Accepting the H0 if the P value is more than 0.05\n",
    "            print(predictor, 'is NOT correlated with', TargetVariable, '| P-Value:', AnovaResults[1])\n",
    "    \n",
    "    return(SelectedPredictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting final predictors for Machine Learning\n",
    "Based on the above tests, selecting the final columns for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SelectedColumns=['Intrinsic Value','Book Value','EPS ','CMP','Graham Price']\n",
    "\n",
    "# Selecting final columns\n",
    "DataForML=SAMPLEDATA[SelectedColumns]\n",
    "DataForML.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving this final data for reference during deployment\n",
    "DataForML.to_pickle('DataForML.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a pickle file\n",
    "DataForML=pd.read_pickle('DataForML.pkl')\n",
    "DataForML.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing for Machine Learning\n",
    "List of steps performed on predictor variables before data can be used for machine learning\n",
    "1. Converting each Ordinal Categorical columns to numeric\n",
    "2. Converting Binary nominal Categorical columns to numeric using 1/0 mapping\n",
    "3. Converting all other nominal categorical columns to numeric using pd.get_dummies()\n",
    "4. Data Transformation (Optional): Standardization/Normalization/log/sqrt. Important if you are using distance based algorithms like KNN, or Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating all the nominal variables at once using dummy variables\n",
    "DataForML_Numeric=pd.get_dummies(DataForML)\n",
    "\n",
    "# Adding Target Variable to the data\n",
    "DataForML_Numeric['Graham Price']=SAMPLEDATA['Graham Price']\n",
    "\n",
    "# Printing sample rows\n",
    "DataForML_Numeric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Target Variable and Predictor Variables\n",
    "TargetVariable='Graham Price'\n",
    "Predictors=['Intrinsic Value','Book Value','EPS ','CMP']\n",
    "\n",
    "# Simple Linear Regression (only one predictor)\n",
    "# Predictors=['']\n",
    "\n",
    "X=DataForML_Numeric[Predictors].values\n",
    "y=DataForML_Numeric[TargetVariable].values\n",
    "\n",
    "# Polynomial Regression\n",
    "# Uncomment below lines if you want to perform polynomial regression\n",
    "#from sklearn.preprocessing import PolynomialFeatures\n",
    "#poly = PolynomialFeatures(degree = 2, include_bias=False)\n",
    "#X = poly.fit_transform(X)\n",
    "#Predictors=poly.get_feature_names()\n",
    "\n",
    "# Split the data into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=41)\n",
    "\n",
    "# Quick check on the shapes of train and test\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "RegModel = LinearRegression()\n",
    "\n",
    "# Printing all the parameters of Linear regression\n",
    "print(RegModel)\n",
    "\n",
    "# Creating the model on Training Data\n",
    "LREG=RegModel.fit(X_train,y_train)\n",
    "\n",
    "# Taking the standardized values to original scale\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "# Measuring Goodness of fit in Training data\n",
    "print('R2 Value:',metrics.r2_score(y_train, LREG.predict(X_train)))\n",
    "\n",
    "###########################################################################\n",
    "print('\\n##### Model Validation and Accuracy Calculations ##########')\n",
    "\n",
    "# Printing some sample values of prediction\n",
    "prediction=LREG.predict(X_test)\n",
    "TestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)\n",
    "TestingDataResults[TargetVariable]=y_test\n",
    "TestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)\n",
    "print(TestingDataResults.head())\n",
    "\n",
    "# Calculating the error for each row\n",
    "TestingDataResults['APE']=100 * ((abs(\n",
    "  TestingDataResults['Graham Price']-TestingDataResults['PredictedGraham Price']))/TestingDataResults['Graham Price'])\n",
    "\n",
    "# Printing sample prediction values\n",
    "print(TestingDataResults[[TargetVariable,'Predicted'+TargetVariable, 'APE']].head())\n",
    "\n",
    "\n",
    "MAPE=np.mean(TestingDataResults['APE'])\n",
    "MedianMAPE=np.median(TestingDataResults['APE'])\n",
    "\n",
    "Accuracy =100 - MAPE\n",
    "MedianAccuracy=100- MedianMAPE\n",
    "print('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier\n",
    "print('Median Accuracy on test data:', MedianAccuracy)\n",
    "\n",
    "\n",
    "# Defining a custom function to calculate accuracy\n",
    "# Make sure there are no zeros in the Target variable if you are using MAPE\n",
    "def Accuracy_Score(orig,pred):\n",
    "    MAPE = np.mean(100 * (np.abs(orig-pred)/orig))\n",
    "    #print('#'*70,'Accuracy:', 100-MAPE)\n",
    "    return(100-MAPE)\n",
    "\n",
    "# Custom Scoring MAPE calculation\n",
    "from sklearn.metrics import make_scorer\n",
    "custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)\n",
    "\n",
    "# Importing cross validation function from sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Running 10-Fold Cross validation on a given algorithm\n",
    "# Passing full data X and y because the K-fold will split the data and automatically choose train/test\n",
    "Accuracy_Values=cross_val_score(RegModel, X , y, cv=5, scoring=custom_Scoring)\n",
    "print('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\n",
    "print('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTIPLE LINNEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the line of best fit\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x=TestingDataResults['CMP'] , y=TestingDataResults['Graham Price'])\n",
    "plt.scatter(TestingDataResults['CMP'] ,TestingDataResults['PredictedGraham Price'], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SelectedColumns=['Intrinsic Value','Book Value','EPS ','CMP']\n",
    "# Looking at the coefficients for each column (M Value)\n",
    "LREG.coef_\n",
    "# Looking at the intercept (C Value)\n",
    "LREG.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees (Multiple if-else statements!)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "RegModel = DecisionTreeRegressor(max_depth=3, criterion='squared_error')\n",
    "# Good Range of hyper parameter Max_depth = 2 to 20\n",
    "\n",
    "# Printing all the parameters of Decision Tree\n",
    "print(RegModel)\n",
    "\n",
    "# Creating the model on Training Data\n",
    "DT=RegModel.fit(X_train,y_train)\n",
    "\n",
    "from sklearn import metrics\n",
    "# Measuring Goodness of fit in Training data\n",
    "print('R2 Value:',metrics.r2_score(y_train, DT.predict(X_train)))\n",
    "\n",
    "# Plotting the feature importance for Top 10 most important columns\n",
    "%matplotlib inline\n",
    "feature_importances = pd.Series(DT.feature_importances_, index=Predictors)\n",
    "feature_importances.nlargest(10).plot(kind='barh')\n",
    "\n",
    "###########################################################################\n",
    "print('\\n##### Model Validation and Accuracy Calculations ##########')\n",
    "prediction=DT.predict(X_test)\n",
    "# Printing some sample values of prediction\n",
    "TestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)\n",
    "TestingDataResults[TargetVariable]=y_test\n",
    "TestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)\n",
    "\n",
    "# Printing sample prediction values\n",
    "print(TestingDataResults[[TargetVariable,'Predicted'+TargetVariable]].head())\n",
    "\n",
    "# Calculating the error for each row\n",
    "TestingDataResults['APE']=100 * ((abs(\n",
    "  TestingDataResults['Graham Price']-TestingDataResults['PredictedGraham Price']))/TestingDataResults['Graham Price'])\n",
    "\n",
    "MAPE=np.mean(TestingDataResults['APE'])\n",
    "MedianMAPE=np.median(TestingDataResults['APE'])\n",
    "\n",
    "Accuracy =100 - MAPE\n",
    "MedianAccuracy=100- MedianMAPE\n",
    "print('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier\n",
    "print('Median Accuracy on test data:', MedianAccuracy)\n",
    "\n",
    "\n",
    "# Defining a custom function to calculate accuracy\n",
    "# Make sure there are no zeros in the Target variable if you are using MAPE\n",
    "def Accuracy_Score(orig,pred):\n",
    "    MAPE = np.mean(100 * (np.abs(orig-pred)/orig))\n",
    "    #print('#'*70,'Accuracy:', 100-MAPE)\n",
    "    return(100-MAPE)\n",
    "\n",
    "# Custom Scoring MAPE calculation\n",
    "from sklearn.metrics import make_scorer\n",
    "custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)\n",
    "\n",
    "# Importing cross validation function from sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Running 10-Fold Cross validation on a given algorithm\n",
    "# Passing full data X and y because the K-fold will split the data and automatically choose train/test\n",
    "Accuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)\n",
    "print('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\n",
    "print('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))\n",
    "\n",
    "\n",
    "# exporting the image to the local disk\n",
    "plt.show()\n",
    "plt.savefig('Bar Charts.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from IPython.display import Image\n",
    "from sklearn import tree\n",
    "import pydotplus\n",
    "\n",
    "# Create DOT data\n",
    "dot_data = tree.export_graphviz(RegModel, out_file=None, \n",
    "                                feature_names=Predictors, class_names=TargetVariable)\n",
    "# printing the rules\n",
    "print(dot_data)\n",
    "\n",
    "# Draw graph\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "# Show graph\n",
    "Image(graph.create_png(), width=1200,height=1500)\n",
    "# Double click on the graph to zoom in\n",
    "\n",
    "\n",
    "# exporting the image to the local disk\n",
    "plt.show()\n",
    "plt.savefig('Decision Tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SelectGraham=['Name','Graham Price','CMP']\n",
    "Graham=SAMPLEDATA[SelectGraham]\n",
    "top_n = Graham.nlargest(10, 'Graham Price')\n",
    "print(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL TESTING RESULT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Selectstock=['Name','Graham Price','CMP','G Factor','Book Value']\n",
    "Stock=SAMPLEDATA[Selectstock]\n",
    "\n",
    "UndervaluedStock = Stock[Stock['Graham Price'] > Stock['CMP']]\n",
    "print(\"Filtered rows where Graham Price> Market Value :\")\n",
    "print(UndervaluedStock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THANK YOU SO MUCH for reading the code           (This Code is writen by Somnath Banerjee)           GMail: somnathbanerjee342000@gmail.com"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
